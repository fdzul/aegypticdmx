---
title: "Nicho Ecológico de *Aedes aegypti* en el área Metropolitana de la Ciudad de México con XGBoost"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Nicho Ecológico de *Aedes aegypti* en el área Metropolitana de la Ciudad de México con XGBoost}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Objetivo

### Predicting the presence of *Aedes aegypti* in the Metropolitan Area of Mexico City using XGBoost

## Load the pkg

```{r setup}
library(aegypticdmx)
## R
library(tidyverse) # Conjunto de paquetes para manejo de datos
library(magrittr) # Pipe
library(tidymodels) # Machine Learning en R
library(skimr) # Descriptivas univariadas masivas
library(bonsai) # LightGBM
## Balanceo
library(themis) # Upsampling - downsampling
## Estos son para hacer computacion en paralelo en Windows
library(parallel)
library(doParallel)

```

## Load the dengue dataset and the area of interes

```{r load_dataset, warning=FALSE, message=FALSE}
# Step 1.1 load the dataset ####
data <- aegypticdmx::ae_aegypti_cdmx

# Step 1.2 load the area of interes ####
aoi <- aegypticdmx::ua_cdmx

```

### map the dataset

```{r map_dengue_cases,  warning=FALSE, message=FALSE}
mapview::mapview(data,
                 layer.name = "Aedes aegypti",
                 zcol = "class") +
    mapview::mapview(aoi,
                     legend = FALSE)
```


### glimpse of your data

```{r,  warning=FALSE, message=FALSE}
data |>
    dplyr::glimpse()
```

### EDA Univariado

```{r,  warning=FALSE, message=FALSE}
# EDA Univariado ####
skimr::skim(data |> sf::st_drop_geometry())
```

### balanceo

#### tabla

```{r balanceo_table, warning=FALSE, message=FALSE }
# Balanceo
data |>
    sf::st_drop_geometry() |>
    dplyr::group_by(class) |> 
    dplyr::count(name = 'frec') |>
    dplyr::ungroup() |>
    dplyr::mutate( Porc= frec/sum(frec)) |>
    gt::gt()

```

#### gráfica

```{r,warning=FALSE, message=FALSE}
data |>
    sf::st_drop_geometry() |>
    dplyr::group_by(class) |> 
    dplyr::count(name = 'frec') |>
    dplyr::ungroup() |>
    dplyr::mutate(Porc= frec/sum(frec)) |>
    ggplot2::ggplot(ggplot2::aes(x= class, 
                                 y= Porc)) +
    ggplot2::geom_segment(ggplot2::aes(xend = class, 
                                       y = 0, 
                                       yend=Porc), 
                          color= c("#4285F4","#E01A59"), 
                          linewidth= 1) +
    ggplot2::geom_point(size=5, 
                        color= c("#4285F4","#E01A59")) +
    ggplot2::coord_flip() +
    ggplot2::scale_y_continuous(labels = scales::percent_format()) +
    ggplot2::labs(title= 'Porcentaje de Registros de Ae. aegypti', 
                  y = "Porcentaje", 
                  x = "") +
    ggplot2::theme_bw()
```

### EDA Multivariado

![](corrmat_data_aedes_aegypti.png)

## Modelamiento

### Train-Test Split

initial split

```{r train_test, warning=FALSE, message=FALSE}
# . Train-Test Split ####
set.seed(12345) # Semilla para aleatorios
split <- data |>
    rsample::initial_split(prop = 0.8, 
                           strata = class)
```

Train

```{r}
train <- rsample::training(split)
dim(train)
```

Test

```{r}
test <- rsample::testing(split)
dim(test)
```

### Preprocesamiento

#### Balancear usando pesos

```{r balanceo, warning=FALSE, message=FALSE}
train <- train |> 
    dplyr::mutate(## crear la variable con los pesos
        case_wts = ifelse(class == "presence", 3, 1),
        ## crea el vector de importancia ponderada
        case_wts = recipes::importance_weights(case_wts))
```

```{r receta, warning=FALSE, message=FALSE}
receta <- train |>
  sf::st_drop_geometry() |>
  recipes::recipe(class ~ . ) |>## Crea la receta
  #recipes::step_indicate_na(urban, tree, ia) |>
    ## Eliminar variables que no usaremos
    # step_rm() |>
  
    ## Crear nuevas variables (insight desde el EDA)
    #recipes::step_mutate(temperature = temperature/1000) |>
    ## Imputar los datos 
    # step_impute_mean()
  recipes::step_impute_knn(recipes::all_predictors() ) |>
    ## Estandarizacion/Normalizacion de numericas
  recipes::step_normalize(recipes::all_numeric(), 
                             -recipes::all_outcomes()) |>
    ## Crear una categoría "otros" que agrupe a categorias pequeñas
  recipes::step_other(recipes::all_nominal(), 
                        -recipes::all_outcomes() , 
                        threshold = 0.07, 
                        other = "otros") |>
    ## Crear una categoría "new" para observaciones con labels "no muestreados"
  recipes::step_novel(recipes::all_nominal(), 
                        -recipes::all_outcomes() , 
                        new_level = "new") |>
    ## Crear variables indicadoras para cada categoría
    # recipes::step_dummy(all_nominal(), -all_outcomes() ) |># Dummy
    ## Eliminar automáticamente variables con alta correlacion 
    ## para evitar la multicolinealidad xi ~ xj
  recipes::step_corr(recipes::all_numeric(),
                       -recipes::all_outcomes(), 
                       threshold = 0.8)
    ## Tambien podemos eliminar variables con multicolinealidad "a mano"
    #recipes::step_rm(suhi_night, suhi_day,wordcover)
```

```{r, warning=FALSE, message=FALSE}
receta

receta |>
    recipes::prep() |>
    recipes::bake(new_data = NULL) |>
    dplyr::glimpse()
```

### Entrenamiento y ajuste de Hiperparámetros

#### Remuestreo

```{r remuestreo, warning=FALSE, message=FALSE}
set.seed(12345)
cv <- rsample::vfold_cv(train |> sf::st_drop_geometry(), 
                        v = 10, 
                        repeats = 2, 
                        strata = class)
cv

```

#### Métricas

```{r metricas,  warning=FALSE, message=FALSE}
#. Métricas ####
library(yardstick)
library(tidysdm)
metricas <- yardstick::metric_set(roc_auc,
                                  yardstick::sens,
                                  yardstick::spec,
                                  bal_accuracy,
                                  pr_auc, 
                                  accuracy, 
                                  boyce_cont, 
                                  tss_max, 
                                  kap)
metricas
```


#### Activar la Paralelizacion

```{r paralelizacion, warning=FALSE, message=FALSE}
parallel::detectCores(logical=FALSE)

cl <- parallel::makePSOCKcluster(8)
doParallel::registerDoParallel(cl)
```

#### Especificación del modelo

```{r esp_mod, warning=FALSE, message=FALSE}
xgb_sp <- parsnip::boost_tree(mtry = tune::tune(), 
                              trees = tune::tune(), 
                              tree_depth = tune::tune(),
                              loss_reduction = tune::tune(), 
                              learn_rate= tune::tune()) |>
    parsnip::set_engine("xgboost") |>
    parsnip::set_mode("classification")
```

#### Workflow

```{r worflow,  warning=FALSE, message=FALSE}
xgb_wflow <- workflows::workflow() |> 
    workflows::add_recipe(receta) |> 
    workflows::add_model(xgb_sp) |> 
    workflows::add_case_weights(case_wts) ## Aquí agregamos los pesos

xgb_wflow 
```

### Afinamiento de hiperparametros

#### Malla de Busqueda

```{r mesh_search, warning=FALSE, message=FALSE}
set.seed(12345)
xgb_grid <- xgb_sp |>
    ## preguntamos los parametros tuneables del modelo
    workflowsets::extract_parameter_set_dials() |>
    ## Vamos a definir un rango para el min_n y mtry
    recipes::update(
        ## 18 columnas sqrt(18) = 4.24 aprox 4, min = 4-1, max = 4 +3
        mtry = dials::mtry(range= c(2, 7)),
        ## Cantidad de árboles en el proceso de boosting
        trees= dials::trees(range = c(20, 1000)),
        ## Profundidad de cada arbol
        tree_depth= dials::tree_depth(range= c(3, 7))
        ## Los otros hiperparam los dejamos por default
    ) |> 
    ## En Boosting se suelen tener mallas grandes, porque estos modelos
    ## son muy sensibles a los hiperparametros y ademas tienen una 
    ## gran cantidad de hiperparametros
    #grid_latin_hypercube(size = 40)
    dials::grid_space_filling(size = 100)

xgb_grid 
```

#### Entrenamiento de Malla de Busqueda con Crossvalidation

```{r entrenamiento_mesh_search, warning=FALSE, message=FALSE}
tictoc::tic()
set.seed(12345)
xgb_tuned <- tune::tune_grid(xgb_wflow, ## Modelo
                             resamples= cv, ## Crossvalidation
                             grid = xgb_grid, ## Malla de Busqueda
                             metrics = metricas, ## Metricas
                             control= tune::control_grid(allow_par = T, 
                                                         save_pred = T))
tictoc::toc()

```

accurary

```{r}
tune::show_best(xgb_tuned, metric = 'accuracy', n = 10)
```

sens

```{r}
tune::show_best(xgb_tuned, metric = 'sens', n = 10)
```

spec

```{r}
tune::show_best(xgb_tuned, metric = 'spec', n = 10)
```

## Modelo Final

### Definir la mejor combinacion de hyperparametros

```{r hyperparametros, warning=FALSE, message=FALSE}
xgb_pars_fin <- tune::select_best(xgb_tuned, 
                                   metric = "sens")
xgb_pars_fin
```

### Finalizar (darle valores a parametros tuneables) el workflow

```{r, warning=FALSE, message=FALSE}
xgb_wflow_fin <- xgb_wflow |>
    tune::finalize_workflow(xgb_pars_fin)
xgb_wflow_fin
```

### Entrenar el modelo final

```{r ent_mod,warning=FALSE, message=FALSE}
xgb_fitted <- fit(xgb_wflow_fin, train)
xgb_fitted
```

### Evaluar el modelo

```{r evaluacion, warning=FALSE, message=FALSE}
library(magrittr)
table <- dplyr::bind_cols(train %>%
                              predict(xgb_fitted , new_data = . ) |>
                              dplyr::mutate(Real= train$class) |>
                              yardstick::conf_mat(truth = Real, 
                                                  estimate = .pred_class ) |>
                              summary() |>
                              dplyr::rename(train = .estimate),
                          test %>%
                              predict(xgb_fitted, new_data = . ) |>
                              dplyr::mutate(Real= test$class) |>
                              yardstick::conf_mat(truth = Real, 
                                                  estimate = .pred_class ) |>
                              summary() |>
                              dplyr::rename(test = .estimate) |>
                              dplyr::select(test)) |>
    dplyr::mutate(difference = train-test) |>
    dplyr::mutate(train = round(train, 2),
                  test = round(test, 2),
                  difference = round(difference, 2))

tss <- table |>
    dplyr::filter(.metric %in% c("sens", "spec")) 


y <- train %>%
    predict(xgb_fitted , new_data = ., type = "prob") |>
    dplyr::mutate(Real= train$class) |>
    dplyr::mutate(Real = ifelse(Real == "presence", 1, 0))

x  <- test %>%
    predict(xgb_fitted , new_data = ., type = "prob") |>
    dplyr::mutate(Real= test$class) |>
    dplyr::mutate(Real = ifelse(Real == "presence", 1, 0)) |>
    dplyr::select(-.pred_pseudoabs)

table |>
    dplyr::bind_rows(tibble::tibble(.metric = c("auc"),
                                    .estimator = c("binary"),
                                    train = round(Metrics::auc(y$Real, y$.pred_presence),2),
                                    test = round(Metrics::auc(x$Real, x$.pred_presence),2),
                                    difference = round(train-test, 2)),
                     tibble::tibble(.metric = c("TSS"),
                                    .estimator = c("binary"),
                                    train = sum(tss$train),
                                    test = sum(tss$test),
                                    difference = round(train-test, 2))) |>
    #dplyr::arrange(dplyr::desc(train)) |>
    gt::gt() |>
    gt::tab_style(style = list(gt::cell_text(weight = "bold")),
                  locations = gt::cells_body(columns = c(.metric, train, test, difference),
                                             rows = .metric %in% c("auc","TSS", "sens", 
                                                           "accuracy", "bal_accuracy")))
```


Correlación Biserial

Train

```{r corr_biserial_train, warning=FALSE, message=FALSE}
y  <- train %>%
    predict(xgb_fitted , new_data = ., type = "prob") |>
    dplyr::mutate(Real= train$class) |>
    dplyr::mutate(Real = ifelse(Real == "presence", 1, 0)) |>
    dplyr::mutate(pred_presence = ifelse(.pred_presence >= .5, 1, 0)) |>
    dplyr::select(-.pred_pseudoabs)

correlation::correlation(data = y,
                         select = "Real",
                         select2 = ".pred_presence",
                         method = "biserial")
```

Test

```{r corr_biserial_test, warning=FALSE, message=FALSE}
x  <- test %>%
    predict(xgb_fitted , new_data = ., type = "prob") |>
    dplyr::mutate(Real= test$class) |>
    dplyr::mutate(Real = ifelse(Real == "presence", 1, 0)) |>
    dplyr::mutate(pred_presence = ifelse(.pred_presence >= .5, 1, 0)) |>
    dplyr::select(-.pred_pseudoabs)
correlation::correlation(data = x,
                         select = "Real",
                         select2 = ".pred_presence",
                         method = "biserial")
```

Correlation Tetrácorica

Train

```{r cor_tetrachoric_train, warning=FALSE, message=FALSE}
correlation::correlation(data = y,
                         select = "Real",
                         select2 = "pred_presence",
                         method = "tetrachoric")
```

Test

```{r cor_tetrachoric_test, warning=FALSE, message=FALSE}
correlation::correlation(data = x,
                         select = "Real",
                         select2 = "pred_presence",
                         method = "tetrachoric")
```

### Detener la paralelización

```{r}
parallel::stopCluster(cl)
```

## Análisis Posteriores

```{r vip, warning=FALSE, message=FALSE}
xgb_fitted |>
workflows::extract_fit_parsnip() |>
    vip::vip(geom= "col",
             num_features = 16,
              aesthetics = list(color = "white",
                                fill = "black",
                                size = 1))
```


## Predicción

### Subir las capas

```{r layers, warning=FALSE, message=FALSE}
library(tidyterra)
layers <- terra::rast(system.file("extdata",
                                       "lyr_cdmx.tif",
                                       package = "aegypticdmx")) |>
    dplyr::mutate(temperature = temperature/1000) |>
    dplyr::select(-suhi_day, -suhi_night, -wordcover) |>
    dplyr::rename(hfp = hfp2022)
```

### Hacer la predicción

```{r pred, warning=FALSE, message=FALSE}
prediction <- tidysdm::predict_raster(object = xgb_fitted, 
                                      raster = layers,
                                      type = "prob")
```

### Extraer la predicciones del area de interes

```{r extraer_pred, warning=FALSE, message=FALSE}
pred <- terra::crop(x = prediction,
                    y = aoi,
                    mask = TRUE)
```

### Visualizar las predicciones

```{r visualizacion, warning=FALSE, message=FALSE}
mapview::mapview(pred,
                 layer.name = "Probabilidad",
                 zcol = ".pred_presence")
```
